{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A3C for Kung Fu"
      ],
      "metadata": {
        "id": "dIo6Zkp7U1Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ],
      "metadata": {
        "id": "pz8ogVxGVB6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "CqN2IEX1VKzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!pip install ale-py\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "BrsNHNQqVZLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Categorical\n",
        "import ale_py\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium import ObservationWrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Building the AI"
      ],
      "metadata": {
        "id": "VF6EFSGUVlk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "qyNc8cxbZCYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  \"\"\"Neural network with conv layers for A3C policy and value predictions.\"\"\"\n",
        "  def __init__(self, action_size):\n",
        "    super(Network, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 4, out_channels = 32, kernel_size = (3, 3), stride = 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3, 3), stride = 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = (3, 3), stride = 2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(512, 128)\n",
        "    self.fc2a = nn.Linear(128, action_size)\n",
        "    self.fc2s = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    # Puts the images through the 3 convolutional layers, flattens the output\n",
        "    # then sends that through the fully connected layers\n",
        "    x = F.relu(self.conv1(state))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.flatten(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    action_values = self.fc2a(x)\n",
        "    state_value = self.fc2s(x)[0]\n",
        "    return action_values, state_value"
      ],
      "metadata": {
        "id": "scJ9LgXA-oHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Training the AI"
      ],
      "metadata": {
        "id": "eF5bETqbZbCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "3C2ydyKLZgaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessAtari(ObservationWrapper):\n",
        "  \"\"\"Preprocesses Atari frames: resize, grayscale, normalize, and frame stacking.\"\"\"\n",
        "  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
        "    super(PreprocessAtari, self).__init__(env)\n",
        "    self.img_size = (height, width)\n",
        "    self.crop = crop\n",
        "    self.dim_order = dim_order\n",
        "    self.color = color\n",
        "    self.frame_stack = n_frames\n",
        "\n",
        "    # Number of channels: 1 per frame if grayscale, 3 per frame if color\n",
        "    n_channels = 3 * n_frames if color else n_frames\n",
        "    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
        "    self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "    self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    self.frames = np.zeros_like(self.frames)\n",
        "    obs, info = self.env.reset()\n",
        "    self.update_buffer(obs)\n",
        "    return self.frames, info\n",
        "\n",
        "  def observation(self, img):\n",
        "    \"\"\"Processing a single frame.\"\"\"\n",
        "    img = self.crop(img)\n",
        "    img = cv2.resize(img, self.img_size)\n",
        "    if not self.color:\n",
        "\n",
        "      # Change to grayscale if needed\n",
        "      if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = img.astype('float32') / 255.\n",
        "\n",
        "    # Shift frames to make space for new ones and store the last frame\n",
        "    if self.color:\n",
        "      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
        "    else:\n",
        "      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
        "    if self.color:\n",
        "      self.frames[-3:] = img\n",
        "    else:\n",
        "      self.frames[-1] = img\n",
        "    return self.frames\n",
        "\n",
        "  def update_buffer(self, obs):\n",
        "    self.frames = self.observation(obs)\n",
        "\n",
        "def make_env():\n",
        "  env = gym.make(\"KungFuMasterNoFrameskip-v4\", render_mode = 'rgb_array')\n",
        "  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
        "  return env\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "state_shape = env.observation_space.shape\n",
        "number_actions = env.action_space.n\n",
        "print(\"State shape:\", state_shape)\n",
        "print(\"Number actions:\", number_actions)\n",
        "print(\"Action names:\", env.env.env.env.get_action_meanings())"
      ],
      "metadata": {
        "id": "gF756uIhRVcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "YgRlooBmC1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "gamma = 0.99\n",
        "number_environments = 10"
      ],
      "metadata": {
        "id": "DulCZ5HNUnmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the A3C class"
      ],
      "metadata": {
        "id": "Gg_LmSs9IoTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  \"\"\"A3C agent: selects actions and updates network using policy gradients.\"\"\"\n",
        "  def __init__(self, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "    self.network = Network(action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.network.parameters(), lr = learning_rate)\n",
        "\n",
        "  def act(self, state):\n",
        "    # Ensure state is batched (1, C, H, W) if a single frame\n",
        "    if state.ndim == 3:\n",
        "      state = [state]\n",
        "    state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
        "    action_values, _ = self.network(state)\n",
        "    policy = F.softmax(action_values, dim = -1)\n",
        "\n",
        "    # Sample actions from the probability distribution\n",
        "    return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu().numpy()])\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    batch_size = state.shape[0]\n",
        "\n",
        "    # Convert everything to torch tensors on correct device\n",
        "    state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
        "    next_state = torch.tensor(next_state, dtype = torch.float32, device = self.device)\n",
        "    reward = torch.tensor(reward, dtype = torch.float32, device = self.device)\n",
        "    done = torch.tensor(done, dtype = torch.bool, device = self.device).to(dtype = torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    action_values, state_value = self.network(state)\n",
        "    _, next_state_value = self.network(next_state)\n",
        "\n",
        "    # Compute target for the critic\n",
        "    target_state_value = reward + gamma * next_state_value * (1 - done)\n",
        "    advantage = target_state_value - state_value\n",
        "    probs = F.softmax(action_values, dim = -1)\n",
        "    logprobs = F.log_softmax(action_values, dim = -1)\n",
        "    entropy = -torch.sum(probs * logprobs, axis = -1)\n",
        "\n",
        "    # Select log-probabilities corresponding to taken actions\n",
        "    batch_idx = np.arange(batch_size)\n",
        "    logp_actions = logprobs[batch_idx, action]\n",
        "\n",
        "    # Calculate the loss\n",
        "    actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean()\n",
        "    critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    self.optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "y9buRYgbVHwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the A3C agent"
      ],
      "metadata": {
        "id": "7RnRukHDKFJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(number_actions)"
      ],
      "metadata": {
        "id": "yiIfJxpluRAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our A3C agent on a single episode"
      ],
      "metadata": {
        "id": "oB5SpmoKP0aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Runs the agent in the environment to return cumulative rewards.\"\"\"\n",
        "def evaluate(agent, env, n_episodes = 1):\n",
        "  episodes_rewards = []\n",
        "  for _ in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "      action = agent.act(state)\n",
        "      next_state, reward, done, info, _ = env.step(action[0])\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    episodes_rewards.append(total_reward)\n",
        "  return episodes_rewards"
      ],
      "metadata": {
        "id": "s_TDKt_9u4IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing multiple agents on multiple environments at the same time"
      ],
      "metadata": {
        "id": "jVSqiyjiQeMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvBatch:\n",
        "  \"\"\"Manages multiple parallel environments for faster A3C training.\"\"\"\n",
        "  def __init__(self, n_envs = 10):\n",
        "    self.envs = [make_env() for _ in range(n_envs)]\n",
        "\n",
        "  def reset(self):\n",
        "    _states = []\n",
        "    for env in self.envs:\n",
        "      _states.append(env.reset()[0])\n",
        "    return np.array(_states)\n",
        "\n",
        "  def step(self, actions):\n",
        "    # Step all environments in the batch and collect the outputs\n",
        "    next_states, rewards, dones, infos, _ = map(np.array, zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
        "    for i in range(len(self.envs)):\n",
        "      if dones[i]:\n",
        "        next_states[i] = self.envs[i].reset()[0]\n",
        "    return next_states, rewards, dones, infos"
      ],
      "metadata": {
        "id": "mXLj5n4Q1br7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the A3C agent"
      ],
      "metadata": {
        "id": "69WZWB4oRx1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "env_batch = EnvBatch(number_environments)\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "with tqdm.trange(0, 3001) as progress_bar:\n",
        "  for i in progress_bar:\n",
        "    batch_actions = agent.act(batch_states)\n",
        "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
        "    batch_rewards *= 0.01\n",
        "    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
        "    batch_states = batch_next_states\n",
        "    if i % 1000 == 0:\n",
        "      print(\"Average agent reward: \", np.mean(evaluate(agent, env, n_episodes = 10)))"
      ],
      "metadata": {
        "id": "OZzEIIke9n7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Visualizing the results"
      ],
      "metadata": {
        "id": "7kG_YR9YdmUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def show_video_of_model(agent, env):\n",
        "  state, _ = env.reset()\n",
        "  done = False\n",
        "  frames = []\n",
        "  while not done:\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    action = agent.act(state)\n",
        "    state, reward, done, _, _ = env.step(action[0])\n",
        "  env.close()\n",
        "  imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, env)\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "UGkTuO6DxZ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i video.mp4 -vf \"fps=20,scale=480:-1:flags=lanczos\" kungfu.gif"
      ],
      "metadata": {
        "id": "BqaCbnicrVEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}